{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2e711c-09f2-4104-9146-4a1de6e64352",
   "metadata": {},
   "source": [
    "# > ***\"Pytorch Training Pipeline***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2299452f-17b2-40e3-93a2-73025035954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d3d73c-6836-407c-ac62-c089de2d0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a01a1d0-501f-4844-8154-c56186339bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst  Unnamed: 32\n",
       "0    842302         M        17.99  ...          0.4601                  0.11890          NaN\n",
       "1    842517         M        20.57  ...          0.2750                  0.08902          NaN\n",
       "2  84300903         M        19.69  ...          0.3613                  0.08758          NaN\n",
       "3  84348301         M        11.42  ...          0.6638                  0.17300          NaN\n",
       "4  84358402         M        20.29  ...          0.2364                  0.07678          NaN\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54bff2-7154-4637-898b-4910add33f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id','Unnamed: 32'],inplace=True) #Drop useless columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9d345-77fc-42fd-a8b7-d5f8489c5a9b",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc795c9-d42a-4f85-8d0e-7bed3ae8d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(df.iloc[:,1:], df.iloc[:,0], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b201be1-f3f7-45d4-92f1-a3a51daa01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc5bb92-af07-43e7-b770-703395064a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42709558,  0.8507601 ,  0.51829101, ...,  1.33615369,\n",
       "         0.5923865 ,  2.22003914],\n",
       "       [ 1.27612231,  0.16784584,  1.13335182, ...,  0.20520683,\n",
       "        -0.54358715, -1.47438797],\n",
       "       [-0.06606846, -0.8390448 , -0.13359252, ..., -0.51676519,\n",
       "        -1.23609725, -0.94443403],\n",
       "       ...,\n",
       "       [-0.35748358,  0.02100762, -0.33874768, ..., -0.26581725,\n",
       "         1.10976064,  0.39325033],\n",
       "       [-0.05486019, -0.48476846, -0.1271182 , ..., -0.56337854,\n",
       "        -0.61428423, -0.99944466],\n",
       "       [ 0.20012804, -0.53837607,  0.13468729, ..., -0.47565374,\n",
       "        -0.80548772, -1.09802808]], shape=(455, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea671b6-5b2e-43eb-9f68-ffbc846a1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89335959-417e-46ff-8f81-bd149a6c674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909274f-c11e-437f-9357-2f546bb0ef6a",
   "metadata": {},
   "source": [
    "**Numpy arrays to Pytorch Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e05b434-a112-47a2-971f-9ba4f0fa4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch. from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d8798b-2ddb-4cb8-a92c-490811515b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.dtype #30 features with 455 values in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16938fe-f399-429b-a2fb-5e90b8ea4fb6",
   "metadata": {},
   "source": [
    "# > ***\"Training the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a1ea92c-0928-4ba0-bdda-26ac3c7081aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN():\n",
    "    def __init__(self,X): #define constructor bro\n",
    "        self.weights = torch.rand(X_train_tensor.shape[1],1,dtype=torch.float64, requires_grad= True)\n",
    "        self.bias = torch.zeros(1,dtype=torch.float64, requires_grad= True)\n",
    "    def forward(self,X):\n",
    "        z = torch.matmul(X, self.weights) + self.bias #z = W*X +b \n",
    "        y_pred = torch.sigmoid(z) #sigmoid due to binary classification\n",
    "        return y_pred\n",
    "    def binary_cross_entropy(self,y_pred, y_true, eps=1e-8):\n",
    "        y_pred = torch.clamp(y_pred, eps, 1 - eps)  # numerical stability\n",
    "        loss = -(y_true * torch.log(y_pred) +\n",
    "                 (1 - y_true) * torch.log(1 - y_pred))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131fe31-da9e-4464-8bce-531691b41488",
   "metadata": {},
   "source": [
    "# ***\"Important Params***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eba535b7-9f74-4660-92ec-c87dd9ce2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca5ae1-8114-4baa-93bd-c9c1dda0c98f",
   "metadata": {},
   "source": [
    "# ***\"Training Pipeline***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ba870c5-8495-4aea-9137-d74eac3697d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.489853609495769\n",
      "Epoch: 2, Loss: 3.340020436467229\n",
      "Epoch: 3, Loss: 3.186780726675728\n",
      "Epoch: 4, Loss: 3.031035911901493\n",
      "Epoch: 5, Loss: 2.870387237781667\n",
      "Epoch: 6, Loss: 2.7067046585928627\n",
      "Epoch: 7, Loss: 2.5420638755875853\n",
      "Epoch: 8, Loss: 2.378601097888241\n",
      "Epoch: 9, Loss: 2.216662181384703\n",
      "Epoch: 10, Loss: 2.055592995774959\n",
      "Epoch: 11, Loss: 1.8967927399010223\n",
      "Epoch: 12, Loss: 1.7451194828730812\n",
      "Epoch: 13, Loss: 1.605496808046784\n",
      "Epoch: 14, Loss: 1.4782644738716655\n",
      "Epoch: 15, Loss: 1.3638956902886206\n",
      "Epoch: 16, Loss: 1.2653604522675113\n",
      "Epoch: 17, Loss: 1.1820227852615923\n",
      "Epoch: 18, Loss: 1.1122692608935196\n",
      "Epoch: 19, Loss: 1.0543119720480816\n",
      "Epoch: 20, Loss: 1.0064738090251055\n",
      "Epoch: 21, Loss: 0.9671692729064181\n",
      "Epoch: 22, Loss: 0.9349323246412878\n",
      "Epoch: 23, Loss: 0.9084530051959114\n",
      "Epoch: 24, Loss: 0.8865844468597467\n",
      "Epoch: 25, Loss: 0.8683417348433339\n",
      "Epoch: 26, Loss: 0.8529116420782374\n",
      "Epoch: 27, Loss: 0.8396579359743341\n",
      "Epoch: 28, Loss: 0.8281052533665084\n",
      "Epoch: 29, Loss: 0.8179069377643066\n",
      "Epoch: 30, Loss: 0.8088107160964659\n",
      "Epoch: 31, Loss: 0.8006303892520863\n",
      "Epoch: 32, Loss: 0.7932252611147457\n",
      "Epoch: 33, Loss: 0.7864860991575293\n",
      "Epoch: 34, Loss: 0.7803258176920789\n",
      "Epoch: 35, Loss: 0.7746733395870022\n",
      "Epoch: 36, Loss: 0.7694695228589896\n",
      "Epoch: 37, Loss: 0.7646644071232955\n",
      "Epoch: 38, Loss: 0.7602153005282374\n",
      "Epoch: 39, Loss: 0.7560854054039503\n",
      "Epoch: 40, Loss: 0.7522427949244634\n",
      "Epoch: 41, Loss: 0.7486596245426064\n",
      "Epoch: 42, Loss: 0.7453115059704113\n",
      "Epoch: 43, Loss: 0.7421769982681026\n",
      "Epoch: 44, Loss: 0.7392371867984632\n",
      "Epoch: 45, Loss: 0.7364753305802905\n",
      "Epoch: 46, Loss: 0.7338765645171619\n",
      "Epoch: 47, Loss: 0.7314276466524019\n",
      "Epoch: 48, Loss: 0.7291167429430903\n",
      "Epoch: 49, Loss: 0.7269332436048467\n",
      "Epoch: 50, Loss: 0.7248676061736747\n",
      "Epoch: 51, Loss: 0.7229112212439672\n",
      "Epoch: 52, Loss: 0.721056297476665\n",
      "Epoch: 53, Loss: 0.7192957629874271\n",
      "Epoch: 54, Loss: 0.7176231806552922\n",
      "Epoch: 55, Loss: 0.7160326752576676\n",
      "Epoch: 56, Loss: 0.7145188706500107\n",
      "Epoch: 57, Loss: 0.7130768354767346\n",
      "Epoch: 58, Loss: 0.7117020361298869\n",
      "Epoch: 59, Loss: 0.7103902958690682\n",
      "Epoch: 60, Loss: 0.7091377591841547\n",
      "Epoch: 61, Loss: 0.7079408606254034\n",
      "Epoch: 62, Loss: 0.7067962974468374\n",
      "Epoch: 63, Loss: 0.7057010055113936\n",
      "Epoch: 64, Loss: 0.7046521379928505\n",
      "Epoch: 65, Loss: 0.7036470464823734\n",
      "Epoch: 66, Loss: 0.7026832641686891\n",
      "Epoch: 67, Loss: 0.701758490812216\n",
      "Epoch: 68, Loss: 0.7008705792764786\n",
      "Epoch: 69, Loss: 0.7000175234161513\n",
      "Epoch: 70, Loss: 0.6991974471512308\n",
      "Epoch: 71, Loss: 0.6984085945821042\n",
      "Epoch: 72, Loss: 0.6976493210214395\n",
      "Epoch: 73, Loss: 0.6969180848365918\n",
      "Epoch: 74, Loss: 0.6962134400111375\n",
      "Epoch: 75, Loss: 0.695534029346692\n",
      "Epoch: 76, Loss: 0.6948785782367675\n",
      "Epoch: 77, Loss: 0.6942458889533575\n",
      "Epoch: 78, Loss: 0.6936348353945258\n",
      "Epoch: 79, Loss: 0.693044358247712\n",
      "Epoch: 80, Loss: 0.6924734605289666\n",
      "Epoch: 81, Loss: 0.6919212034630139\n",
      "Epoch: 82, Loss: 0.6913867026730829\n",
      "Epoch: 83, Loss: 0.6908691246529143\n",
      "Epoch: 84, Loss: 0.6903676834963517\n",
      "Epoch: 85, Loss: 0.6898816378625373\n",
      "Epoch: 86, Loss: 0.6894102881569948\n",
      "Epoch: 87, Loss: 0.6889529739108734\n",
      "Epoch: 88, Loss: 0.6885090713423608\n",
      "Epoch: 89, Loss: 0.6880779910858142\n",
      "Epoch: 90, Loss: 0.6876591760755099\n",
      "Epoch: 91, Loss: 0.6872520995721113\n",
      "Epoch: 92, Loss: 0.6868562633210364\n",
      "Epoch: 93, Loss: 0.6864711958328406\n",
      "Epoch: 94, Loss: 0.6860964507766062\n",
      "Epoch: 95, Loss: 0.6857316054780807\n",
      "Epoch: 96, Loss: 0.6853762595150046\n",
      "Epoch: 97, Loss: 0.6850300334026844\n",
      "Epoch: 98, Loss: 0.6846925673634388\n",
      "Epoch: 99, Loss: 0.6843635201740428\n",
      "Epoch: 100, Loss: 0.6840425680857695\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = MyNN(X_train_tensor)\n",
    "#forward pass\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model.forward(X_train_tensor)\n",
    "    #loss calulcation\n",
    "    loss = model.binary_cross_entropy(y_pred,y_train_tensor)\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #Update parameters\n",
    "    \n",
    "    with torch.no_grad(): #We do not need requires_grad = True as we are just updating the params\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    model.weights.grad.zero_() #To avoid accumulation of previos grads values\n",
    "    model.bias.grad.zero_()\n",
    "    \n",
    "    #print loss in each loop\n",
    "    print(f'Epoch: {epoch +1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d5d7f3e-7c5e-4310-8e7e-fd7eea08d18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.640350878238678\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "with torch.no_grad():\n",
    "    y_pred = model.forward(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.6).float()\n",
    "    accuracy = (y_pred == y_test_tensor). float().mean()\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f7aa6-ee13-4ef6-976d-84e67d07f3ac",
   "metadata": {},
   "source": [
    "# ***\"New class with 2 hidden neurons***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03fd9085-ec70-49fe-83b5-09bd3c7bf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNewNN():\n",
    "    def __init__(self, X):\n",
    "        #Layer 1 with 2 neurons\n",
    "        self.W1 = torch.rand(X_train_tensor[1], 2, \n",
    "                             dtype=torch.float64, requires_grad= True\n",
    "                            )\n",
    "        self.b1 = torch.rand(2, dtype=torch.float64, requires_grad= True)\n",
    "        #Layer 2nd with 1 output neuron\n",
    "        self.W2 = torch.rand(2,1,\n",
    "                           dtype=torch.float64, requires_grad= True)\n",
    "        self.b2 = torch.rand(1,dtype=torch.float64, requires_grad= True )\n",
    "        self.weights = [self.W1, self.W2] #storing in a list so that we don;t have to explicitly call each weight\n",
    "        self.bias = [self.b1, self.b2] #same for bias as well\n",
    "        \n",
    "\n",
    "    def forward(self,X):\n",
    "        h = torch.sigmoid(torch.matmul(X,self.W1) +self.b1)\n",
    "        z = torch.matmul(h, self.W2) + self.b2\n",
    "        y_pred = torch.sigmoid(h)\n",
    "        return y_pred\n",
    "    def binary_cross_entropy(self, y_pred, y_true, eps=1e-8):\n",
    "        y_pred = torch.clamp(y_pred, eps, 1 - eps)\n",
    "        loss = -(y_true * torch.log(y_pred) +\n",
    "                 (1 - y_true) * torch.log(1 - y_pred))\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f12e8096-a211-4faf-bb43-dcdf7a5fae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.5924999363330747\n",
      "Epoch: 2, Loss: 3.446142985846101\n",
      "Epoch: 3, Loss: 3.2994533613030956\n",
      "Epoch: 4, Loss: 3.1482156746908485\n",
      "Epoch: 5, Loss: 2.9925577271989683\n",
      "Epoch: 6, Loss: 2.835038415011099\n",
      "Epoch: 7, Loss: 2.6686927465298185\n",
      "Epoch: 8, Loss: 2.502038261813637\n",
      "Epoch: 9, Loss: 2.335452740046985\n",
      "Epoch: 10, Loss: 2.166280833402894\n",
      "Epoch: 11, Loss: 2.0007224699176662\n",
      "Epoch: 12, Loss: 1.843305993090334\n",
      "Epoch: 13, Loss: 1.694634458338172\n",
      "Epoch: 14, Loss: 1.5492895943841989\n",
      "Epoch: 15, Loss: 1.4167620367114084\n",
      "Epoch: 16, Loss: 1.2984938117448674\n",
      "Epoch: 17, Loss: 1.195699521353801\n",
      "Epoch: 18, Loss: 1.1090868564497802\n",
      "Epoch: 19, Loss: 1.0385450335248743\n",
      "Epoch: 20, Loss: 0.9829699580201212\n",
      "Epoch: 21, Loss: 0.9403434211137338\n",
      "Epoch: 22, Loss: 0.9080486727590877\n",
      "Epoch: 23, Loss: 0.8833792364738329\n",
      "Epoch: 24, Loss: 0.8640392441864898\n",
      "Epoch: 25, Loss: 0.8483427602806025\n",
      "Epoch: 26, Loss: 0.8351470325597703\n",
      "Epoch: 27, Loss: 0.8237066691571081\n",
      "Epoch: 28, Loss: 0.8135434483946653\n",
      "Epoch: 29, Loss: 0.8043514253144136\n",
      "Epoch: 30, Loss: 0.7959331082150899\n",
      "Epoch: 31, Loss: 0.7881583159679441\n",
      "Epoch: 32, Loss: 0.7809383476031383\n",
      "Epoch: 33, Loss: 0.7742100498890327\n",
      "Epoch: 34, Loss: 0.7679261083812885\n",
      "Epoch: 35, Loss: 0.7620491777060422\n",
      "Epoch: 36, Loss: 0.7565483477104825\n",
      "Epoch: 37, Loss: 0.7513970162197634\n",
      "Epoch: 38, Loss: 0.7465716024916295\n",
      "Epoch: 39, Loss: 0.7420507607457768\n",
      "Epoch: 40, Loss: 0.7378148906426673\n",
      "Epoch: 41, Loss: 0.7338458244314163\n",
      "Epoch: 42, Loss: 0.7301266198643603\n",
      "Epoch: 43, Loss: 0.7266414171431058\n",
      "Epoch: 44, Loss: 0.7233753352699679\n",
      "Epoch: 45, Loss: 0.7203143931701066\n",
      "Epoch: 46, Loss: 0.7174454467778385\n",
      "Epoch: 47, Loss: 0.714756136691356\n",
      "Epoch: 48, Loss: 0.712234843014062\n",
      "Epoch: 49, Loss: 0.7098706452089453\n",
      "Epoch: 50, Loss: 0.7076532855344824\n",
      "Epoch: 51, Loss: 0.7055731351004209\n",
      "Epoch: 52, Loss: 0.703621161889905\n",
      "Epoch: 53, Loss: 0.701788900303683\n",
      "Epoch: 54, Loss: 0.7000684219286382\n",
      "Epoch: 55, Loss: 0.6984523073376425\n",
      "Epoch: 56, Loss: 0.6969336188033045\n",
      "Epoch: 57, Loss: 0.69550587386242\n",
      "Epoch: 58, Loss: 0.6941630197060522\n",
      "Epoch: 59, Loss: 0.6928994083958306\n",
      "Epoch: 60, Loss: 0.6917097729230534\n",
      "Epoch: 61, Loss: 0.690589204135703\n",
      "Epoch: 62, Loss: 0.6895331285613455\n",
      "Epoch: 63, Loss: 0.6885372871525857\n",
      "Epoch: 64, Loss: 0.6875977149775205\n",
      "Epoch: 65, Loss: 0.6867107218715232\n",
      "Epoch: 66, Loss: 0.6858728740594943\n",
      "Epoch: 67, Loss: 0.6850809767501115\n",
      "Epoch: 68, Loss: 0.6843320576960636\n",
      "Epoch: 69, Loss: 0.6836233517071452\n",
      "Epoch: 70, Loss: 0.6829522860966468\n",
      "Epoch: 71, Loss: 0.6823164670358601\n",
      "Epoch: 72, Loss: 0.681713666786795\n",
      "Epoch: 73, Loss: 0.681141811779406\n",
      "Epoch: 74, Loss: 0.6805989714967121\n",
      "Epoch: 75, Loss: 0.6800833481291219\n",
      "Epoch: 76, Loss: 0.6795932669579584\n",
      "Epoch: 77, Loss: 0.6791271674275489\n",
      "Epoch: 78, Loss: 0.6786835948651863\n",
      "Epoch: 79, Loss: 0.6782611928087171\n",
      "Epoch: 80, Loss: 0.6778586959023644\n",
      "Epoch: 81, Loss: 0.6774749233225695\n",
      "Epoch: 82, Loss: 0.6771087726970695\n",
      "Epoch: 83, Loss: 0.6767592144820491\n",
      "Epoch: 84, Loss: 0.6764252867639532\n",
      "Epoch: 85, Loss: 0.6761060904543763\n",
      "Epoch: 86, Loss: 0.6758007848483165\n",
      "Epoch: 87, Loss: 0.6755085835179537\n",
      "Epoch: 88, Loss: 0.6752287505159655\n",
      "Epoch: 89, Loss: 0.6749605968641992\n",
      "Epoch: 90, Loss: 0.6747034773052633\n",
      "Epoch: 91, Loss: 0.6744567872962758\n",
      "Epoch: 92, Loss: 0.6742199602255872\n",
      "Epoch: 93, Loss: 0.6739924648348016\n",
      "Epoch: 94, Loss: 0.6737738028298258\n",
      "Epoch: 95, Loss: 0.6735635066659857\n",
      "Epoch: 96, Loss: 0.673361137493478\n",
      "Epoch: 97, Loss: 0.6731662832505586\n",
      "Epoch: 98, Loss: 0.6729785568929114\n",
      "Epoch: 99, Loss: 0.6727975947486091\n",
      "Epoch: 100, Loss: 0.672623054988966\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = MyNN(X_train_tensor)\n",
    "#forward pass\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model.forward(X_train_tensor)\n",
    "    #loss calulcation\n",
    "    loss = model.binary_cross_entropy(y_pred,y_train_tensor)\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    #Update parameters\n",
    "    \n",
    "    with torch.no_grad(): #We do not need requires_grad = True as we are just updating the params\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    model.weights.grad.zero_() #To avoid accumulation of previos grads values\n",
    "    model.bias.grad.zero_()\n",
    "    \n",
    "    #print loss in each loop\n",
    "    print(f'Epoch: {epoch +1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5732439-7252-489d-bf4f-4c25eca9d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6354262828826904\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model.forward(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.6).float()\n",
    "    accuracy = (y_pred == y_test_tensor). float().mean()\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966ba0f-96c5-4c0c-a782-8dea2f064b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
